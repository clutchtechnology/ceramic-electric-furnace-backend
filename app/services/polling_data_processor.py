# ============================================================
# æ–‡ä»¶è¯´æ˜: polling_data_processor.py - æ•°æ®å¤„ç†å’Œç¼“å­˜ç®¡ç†
# ============================================================
# åŠŸèƒ½:
#   1. è§£æå™¨å’Œè½¬æ¢å™¨åˆå§‹åŒ–
#   2. å†…å­˜ç¼“å­˜ç®¡ç† (æœ€æ–°æ•°æ®ä¾›APIè¯»å–)
#   3. æ‰¹é‡å†™å…¥ç¼“å­˜ (åŒé€Ÿè½®è¯¢æ¶æ„)
#   4. æ•°æ®å¤„ç†å‡½æ•° (_process_*)
#   5. è¶é˜€çŠ¶æ€é˜Ÿåˆ—ç®¡ç†
#   6. æ‰¹é‡å†™å…¥ InfluxDB
# ============================================================

import threading
import traceback
from datetime import datetime, timezone
from typing import Optional, Dict, Any, List
from collections import deque

from app.core.influxdb import write_points_batch, build_point
from app.plc.parser_config_db32 import ConfigDrivenDB32Parser
from app.plc.parser_config_db1 import ConfigDrivenDB1Parser
from app.plc.parser_status import ModbusStatusParser
from app.plc.parser_status_db41 import DataStateParser
from app.tools.converter_furnace import FurnaceConverter
from app.tools.converter_elec_db1_simple import (
    convert_db1_arc_data_simple,
    convert_to_influx_fields_simple,
    convert_to_influx_fields_with_change_detection,
    ArcDataSimple,
)
from app.services.feeding_service import get_batch_feeding_total
from app.services.feeding_accumulator import get_feeding_accumulator


# ============================================================
# è§£æå™¨ä¸è½¬æ¢å™¨å®ä¾‹
# ============================================================
_modbus_parser: Optional[ConfigDrivenDB32Parser] = None  # DB32 ä¼ æ„Ÿå™¨è§£æå™¨
_db1_parser: Optional[ConfigDrivenDB1Parser] = None      # DB1 å¼§æµå¼§å‹è§£æå™¨
_status_parser: Optional[ModbusStatusParser] = None       # DB30 çŠ¶æ€è§£æå™¨
_db41_parser: Optional[DataStateParser] = None            # DB41 æ•°æ®çŠ¶æ€è§£æå™¨
_furnace_converter: Optional[FurnaceConverter] = None     # æ•°æ®è½¬æ¢å™¨

# ============================================================
# å†…å­˜ç¼“å­˜ (ä¾› API ç›´æ¥è¯»å–)
# ============================================================
_data_lock = threading.Lock()

# æœ€æ–°ä¼ æ„Ÿå™¨æ•°æ®ç¼“å­˜ (DB32)
_latest_modbus_data: Dict[str, Any] = {}
_latest_modbus_timestamp: Optional[datetime] = None

# æœ€æ–°å¼§æµå¼§å‹ç¼“å­˜ (DB1)
_latest_arc_data: Dict[str, Any] = {}
_latest_arc_timestamp: Optional[datetime] = None

# æœ€æ–°é€šä¿¡çŠ¶æ€ç¼“å­˜ (DB30)
_latest_status_data: Dict[str, Any] = {}
_latest_status_timestamp: Optional[datetime] = None

# æœ€æ–°æ•°æ®çŠ¶æ€ç¼“å­˜ (DB41)
_latest_db41_data: Dict[str, Any] = {}
_latest_db41_timestamp: Optional[datetime] = None

# æœ€æ–°æ–™ä»“é‡é‡ç¼“å­˜ (Modbus RTU)
_latest_weight_data: Dict[str, Any] = {}
_latest_weight_timestamp: Optional[datetime] = None

# ============================================================
# è®¾å®šå€¼å˜åŒ–æ£€æµ‹ç¼“å­˜ (ç”¨äºæ™ºèƒ½å†™å…¥æ•°æ®åº“)
# ============================================================
# ä¸Šä¸€æ¬¡çš„è®¾å®šå€¼ (U, V, W)
_prev_setpoints: Optional[tuple] = None
# ä¸Šä¸€æ¬¡çš„æ‰‹åŠ¨æ­»åŒºç™¾åˆ†æ¯”
_prev_deadzone: Optional[float] = None

# ============================================================
# è¶é˜€çŠ¶æ€é˜Ÿåˆ—ç¼“å­˜ (Valve Status Queue Cache)
# ============================================================
# æ¯ä¸ªè¶é˜€ç»´æŠ¤ä¸€ä¸ªé˜Ÿåˆ—ï¼Œå­˜å‚¨æœ€è¿‘100æ¬¡çš„å¼€å…³çŠ¶æ€
# çŠ¶æ€æ ¼å¼: "10" (å…³é—­), "01" (æ‰“å¼€), "11" (å¼‚å¸¸), "00" (æœªçŸ¥)
_valve_status_queues: Dict[int, deque] = {
    1: deque(maxlen=100),  # è¶é˜€1çŠ¶æ€é˜Ÿåˆ—
    2: deque(maxlen=100),  # è¶é˜€2çŠ¶æ€é˜Ÿåˆ—
    3: deque(maxlen=100),  # è¶é˜€3çŠ¶æ€é˜Ÿåˆ—
    4: deque(maxlen=100),  # è¶é˜€4çŠ¶æ€é˜Ÿåˆ—
}
_valve_status_timestamps: Dict[int, deque] = {
    1: deque(maxlen=100),
    2: deque(maxlen=100),
    3: deque(maxlen=100),
    4: deque(maxlen=100),
}

# ============================================================
# æ‰¹é‡å†™å…¥ç¼“å­˜ (åŒé€Ÿè½®è¯¢æ¶æ„)
# ============================================================
# ğŸ”¥ å¼§æµå¼§å‹ç¼“å­˜ (é«˜é¢‘å†™å…¥)
# - è½®è¯¢é—´éš”: 0.2s
# - æ‰¹é‡å¤§å°: 20æ¬¡ (0.2sÃ—20=4så†™å…¥ä¸€æ¬¡)
_arc_buffer: deque = deque(maxlen=500)
_arc_buffer_count = 0
_arc_batch_size = 20  # 20æ¬¡å¼§æµè½®è¯¢åæ‰¹é‡å†™å…¥ (0.2sÃ—20=4s)

# ğŸ“Š æ™®é€šæ•°æ®ç¼“å­˜ (å¸¸è§„å†™å…¥)
# - è½®è¯¢é—´éš”: 5s
# - æ‰¹é‡å¤§å°: 20æ¬¡ (5sÃ—20=100så†™å…¥ä¸€æ¬¡)
_normal_buffer: deque = deque(maxlen=1000)
_normal_buffer_count = 0
_normal_batch_size = 20  # 20æ¬¡å¸¸è§„è½®è¯¢åæ‰¹é‡å†™å…¥ (5sÃ—20=100s)

# ============================================================
# ç»Ÿè®¡ä¿¡æ¯
# ============================================================
_stats = {
    "total_polls": 0,
    "successful_writes": 0,
    "failed_writes": 0,
    "last_poll_time": None,
    "db32_errors": 0,
    "db1_errors": 0,
    "modbus_errors": 0,
}


# ============================================================
# è§£æå™¨åˆå§‹åŒ–
# ============================================================
def init_parsers():
    """åˆå§‹åŒ–è§£æå™¨"""
    global _modbus_parser, _db1_parser, _status_parser, _db41_parser, _furnace_converter
    
    if _modbus_parser is None:
        try:
            _modbus_parser = ConfigDrivenDB32Parser()
            print("âœ… DB32 ä¼ æ„Ÿå™¨æ•°æ®è§£æå™¨å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"âŒ DB32 è§£æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    if _db1_parser is None:
        try:
            _db1_parser = ConfigDrivenDB1Parser()
            print("âœ… DB1 å¼§æµå¼§å‹è§£æå™¨å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"âŒ DB1 è§£æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    if _status_parser is None:
        try:
            _status_parser = ModbusStatusParser()
            print("âœ… DB30 çŠ¶æ€è§£æå™¨å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"âŒ DB30 è§£æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    if _db41_parser is None:
        try:
            _db41_parser = DataStateParser()
            print("âœ… DB41 æ•°æ®çŠ¶æ€è§£æå™¨å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"âŒ DB41 è§£æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            
    if _furnace_converter is None:
        _furnace_converter = FurnaceConverter()
        print("âœ… ç”µç‚‰æ•°æ®è½¬æ¢å™¨å·²åˆå§‹åŒ–")


def get_parsers():
    """è·å–è§£æå™¨å®ä¾‹ï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ï¼‰
    
    Returns:
        tuple: (db1_parser, modbus_parser, status_parser, db41_parser)
        
    æ³¨æ„: polling_loops_v2.py ä½¿ç”¨å…ƒç»„æ ¼å¼è°ƒç”¨æ­¤å‡½æ•°
    """
    return _db1_parser, _modbus_parser, _status_parser, _db41_parser


def get_parsers_dict():
    """è·å–è§£æå™¨å®ä¾‹ï¼ˆå­—å…¸æ ¼å¼ï¼‰
    
    Returns:
        dict: åŒ…å«æ‰€æœ‰è§£æå™¨å’Œè½¬æ¢å™¨çš„å­—å…¸
    """
    return {
        'db32_parser': _modbus_parser,
        'db1_parser': _db1_parser,
        'db30_parser': _status_parser,
        'db41_parser': _db41_parser,
        'converter': _furnace_converter
    }


# ============================================================
# æ•°æ®å¤„ç†å‡½æ•°
# ============================================================
def process_modbus_data(raw_data: bytes):
    """å¤„ç† DB32 ä¼ æ„Ÿå™¨æ•°æ®
    
    æ•°æ®åŒ…å«: çº¢å¤–æµ‹è·, å‹åŠ›, æµé‡, è¶é˜€çŠ¶æ€
    æ–°å¢: å†·å´æ°´æµé‡è®¡ç®— (0.5sè½®è¯¢, 15ç§’ç´¯è®¡)
    """
    global _latest_modbus_data, _latest_modbus_timestamp
    
    if not _modbus_parser:
        return

    try:
        # 1. è§£æåŸå§‹æ•°æ®
        parsed = _modbus_parser.parse_all(raw_data)
        
        # ========================================
        # 2. å†·å´æ°´æµé‡è®¡ç®— (æ–°å¢é€»è¾‘)
        # ========================================
        from app.services.cooling_water_calculator import get_cooling_water_calculator
        cooling_calc = get_cooling_water_calculator()
        
        # æå–å†·å´æ°´æ•°æ®
        # æ˜ å°„å…³ç³»:
        # - WATER_FLOW_1 (offset 16) -> ç‚‰çš®æµé‡
        # - WATER_FLOW_2 (offset 18) -> ç‚‰ç›–æµé‡
        # - WATER_PRESS_1 (offset 12) -> ç‚‰çš®æ°´å‹ (è¿‡æ»¤å™¨è¿›å£)
        # - WATER_PRESS_2 (offset 14) -> ç‚‰ç›–æ°´å‹ (è¿‡æ»¤å™¨å‡ºå£)
        cooling_flows = parsed.get('cooling_flows', {})
        cooling_pressures = parsed.get('cooling_pressures', {})
        
        # æµé‡æå– (mÂ³/h)
        flow_1_data = cooling_flows.get('WATER_FLOW_1', {})
        flow_2_data = cooling_flows.get('WATER_FLOW_2', {})
        furnace_shell_flow = flow_1_data.get('flow', 0.0) if isinstance(flow_1_data, dict) else 0.0
        furnace_cover_flow = flow_2_data.get('flow', 0.0) if isinstance(flow_2_data, dict) else 0.0
        
        # å‹åŠ›æå– (åŸå§‹å•ä½ Ã—0.01 kPa)
        press_1_data = cooling_pressures.get('WATER_PRESS_1', {})
        press_2_data = cooling_pressures.get('WATER_PRESS_2', {})
        furnace_shell_pressure = press_1_data.get('pressure', 0.0) if isinstance(press_1_data, dict) else 0.0
        furnace_cover_pressure = press_2_data.get('pressure', 0.0) if isinstance(press_2_data, dict) else 0.0
        
        # æ·»åŠ æµ‹é‡æ•°æ®å¹¶è·å–å‹å·®
        cooling_result = cooling_calc.add_measurement(
            furnace_cover_flow=furnace_cover_flow,
            furnace_shell_flow=furnace_shell_flow,
            furnace_cover_pressure=furnace_cover_pressure,
            furnace_shell_pressure=furnace_shell_pressure,
        )
        
        # è®¡ç®—åçš„å‹å·®å­˜å…¥ parsed ä¾›åç»­ä½¿ç”¨
        parsed['filter_pressure_diff'] = {
            'value': cooling_result['pressure_diff'],
            'unit': 'kPa'
        }
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è®¡ç®—ç´¯è®¡æµé‡ (æ¯15ç§’)
        if cooling_result['should_calc_volume']:
            volume_result = cooling_calc.calculate_volume_increment()
            # æ›´æ–°ç´¯è®¡æµé‡åˆ° parsed
            parsed['furnace_cover_total_volume'] = volume_result['furnace_cover_total']
            parsed['furnace_shell_total_volume'] = volume_result['furnace_shell_total']
        else:
            # ä½¿ç”¨ç¼“å­˜çš„ç´¯è®¡å€¼
            volumes = cooling_calc.get_total_volumes()
            parsed['furnace_cover_total_volume'] = volumes['furnace_cover']
            parsed['furnace_shell_total_volume'] = volumes['furnace_shell']
        
        # 3. æ›´æ–°å†…å­˜ç¼“å­˜ (ä¾›å®æ—¶APIä½¿ç”¨)
        with _data_lock:
            _latest_modbus_data = parsed
            _latest_modbus_timestamp = datetime.now()
            
            # ========================================
            # è¶é˜€çŠ¶æ€é˜Ÿåˆ—æ›´æ–°é€»è¾‘ (æ—§ç‰ˆ - ä»…ç”¨äºå†å²è®°å½•API)
            # ========================================
            valve_status_data = parsed.get('valve_status', {})
            valve_status_byte = valve_status_data.get('raw_byte', 0)
            timestamp = datetime.now(timezone.utc)
            
            # è§£ææ¯ä¸ªè¶é˜€çš„2-bitçŠ¶æ€
            for valve_id in range(1, 5):  # è¶é˜€1-4
                bit_offset = (valve_id - 1) * 2
                bit_close = (valve_status_byte >> bit_offset) & 0x01
                bit_open = (valve_status_byte >> (bit_offset + 1)) & 0x01
                
                # ç»„åˆæˆçŠ¶æ€å­—ç¬¦ä¸²: "10"(å…³), "01"(å¼€), "11"(å¼‚å¸¸), "00"(æœªçŸ¥)
                status = f"{bit_close}{bit_open}"
                
                # æ·»åŠ åˆ°é˜Ÿåˆ—
                _valve_status_queues[valve_id].append(status)
                _valve_status_timestamps[valve_id].append(timestamp.isoformat())
        
        # ========================================
        # 4. è¶é˜€å¼€åº¦è®¡ç®—æœåŠ¡ (æ–°å¢ - æ»‘åŠ¨çª—å£ + è‡ªåŠ¨æ ¡å‡†)
        # ========================================
        try:
            from app.services.valve_calculator_service import batch_add_valve_statuses
            valve_status_data = parsed.get('valve_status', {})
            valve_status_byte = valve_status_data.get('raw_byte', 0)
            batch_add_valve_statuses(valve_status_byte, datetime.now(timezone.utc))
        except Exception as valve_err:
            print(f"âš ï¸ è¶é˜€å¼€åº¦è®¡ç®—å¤±è´¥: {valve_err}")
        
        # 5. è½¬æ¢ä¸º InfluxDB Points (ä¾›å†å²å­˜å‚¨)
        # é‡è¦: åªæœ‰åœ¨æœ‰æ‰¹æ¬¡å·æ—¶æ‰å†™å…¥æ•°æ®åº“ï¼Œé¿å…äº§ç”Ÿæ— æ‰¹æ¬¡çš„æ‚ä¹±æ•°æ®
        now = datetime.now(timezone.utc)
        
        # è·å–å½“å‰æ‰¹æ¬¡å· (ä»…ç”±å‰ç«¯æä¾›ï¼Œåç«¯ä¸è‡ªåŠ¨ç”Ÿæˆ)
        from app.services.polling_service import ensure_batch_code
        batch_code = ensure_batch_code()
        
        # åªæœ‰åœ¨æœ‰æ‰¹æ¬¡å·æ—¶æ‰å†™å…¥å†å²æ•°æ®åº“
        if batch_code and _furnace_converter:
            dict_points = _furnace_converter.convert_to_points(parsed, now, batch_code)
            _normal_buffer.extend(dict_points)
            
            # ========================================
            # 6. æ·»åŠ å†·å´æ°´ç´¯è®¡é‡ Point (ç”¨äºå†å²æŸ¥è¯¢)
            # ========================================
            water_point = {
                'measurement': 'sensor_data',
                'tags': {
                    'device_type': 'electric_furnace',
                    'module_type': 'cooling_water_total',
                    'device_id': 'furnace_1',
                    'batch_code': batch_code
                },
                'fields': {
                    'furnace_shell_water_total': parsed.get('furnace_shell_total_volume', 0.0),
                    'furnace_cover_water_total': parsed.get('furnace_cover_total_volume', 0.0),
                },
                'time': now
            }
            _normal_buffer.append(water_point)
            
    except Exception as e:
        print(f"âŒ å¤„ç† DB32 æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def process_arc_data(raw_data: bytes, batch_code: str):
    """å¤„ç† DB1 å¼§æµå¼§å‹æ•°æ® (ç¼“å­˜ + å†™å…¥æ•°æ®åº“)
    
    è®¾å®šå€¼å’Œæ­»åŒºä»…åœ¨å˜åŒ–æ—¶æ‰å†™å…¥æ•°æ®åº“
    
    Args:
        raw_data: DB1 åŸå§‹å­—èŠ‚æ•°æ®
        batch_code: å½“å‰æ‰¹æ¬¡å·
    """
    global _latest_arc_data, _latest_arc_timestamp
    global _prev_setpoints, _prev_deadzone
    
    if not _db1_parser:
        return

    try:
        # 1. è§£æåŸå§‹æ•°æ®
        parsed = _db1_parser.parse_all(raw_data)
        
        # 2. ä½¿ç”¨ç®€åŒ–è½¬æ¢å™¨ (ç›´æ¥ä½¿ç”¨åŸå§‹å€¼)
        arc_data_obj: ArcDataSimple = convert_db1_arc_data_simple(parsed)
        
        # 3. æ„å»ºç¼“å­˜æ•°æ® (UVWä¸‰ç›¸ + ä¸‰ä¸ªè®¾å®šå€¼ + æ‰‹åŠ¨æ­»åŒº)
        setpoints = arc_data_obj.get_setpoints_A()
        arc_cache = {
            'parsed': parsed,
            'converted': arc_data_obj.to_dict(),
            'arc_current': {
                'U': arc_data_obj.phase_U.current_A,
                'V': arc_data_obj.phase_V.current_A,
                'W': arc_data_obj.phase_W.current_A,
            },
            'arc_voltage': {
                'U': arc_data_obj.phase_U.voltage_V,
                'V': arc_data_obj.phase_V.voltage_V,
                'W': arc_data_obj.phase_W.voltage_V,
            },
            'setpoints': {
                'U': setpoints[0],
                'V': setpoints[1],
                'W': setpoints[2],
            },
            'manual_deadzone_percent': arc_data_obj.manual_deadzone_percent,
            'timestamp': arc_data_obj.timestamp
        }
        
        # 4. æ›´æ–°å†…å­˜ç¼“å­˜
        with _data_lock:
            _latest_arc_data = arc_cache
            _latest_arc_timestamp = datetime.now()
        
        # 5. ä½¿ç”¨å˜åŒ–æ£€æµ‹è½¬æ¢ä¸º InfluxDB å­—æ®µ
        now = datetime.now(timezone.utc)
        change_result = convert_to_influx_fields_with_change_detection(
            arc_data_obj, _prev_setpoints, _prev_deadzone
        )
        arc_fields = change_result['fields']
        
        # æ›´æ–°ä¸Šä¸€æ¬¡çš„å€¼
        _prev_setpoints = change_result['current_setpoints']
        _prev_deadzone = change_result['current_deadzone']
        
        if arc_fields:
            # æ·»åŠ ä¸‹æ–™æ€»é‡ (éœ€è¦ä»æ‰¹æ¬¡ä¿¡æ¯è·å– start_time)
            try:
                from app.services.polling_service import get_batch_info
                batch_info = get_batch_info()
                start_time_str = batch_info.get('start_time')
                if start_time_str:
                    from datetime import datetime as dt
                    start_time = dt.fromisoformat(start_time_str)
                    feeding_total = get_batch_feeding_total(batch_code, start_time)
                else:
                    feeding_total = 0.0
            except Exception as feed_err:
                print(f"âš ï¸ è·å–æŠ•æ–™æ€»é‡å¤±è´¥: {feed_err}")
                feeding_total = 0.0
            arc_fields['feeding_total'] = feeding_total
            
            point_dict = {
                'measurement': 'sensor_data',
                'tags': {
                    'device_type': 'electric_furnace',
                    'module_type': 'arc_data',
                    'device_id': 'electrode',
                    'batch_code': batch_code
                },
                'fields': arc_fields,
                'time': now
            }
            _arc_buffer.append(point_dict)
            
            # æ—¥å¿—ï¼šæ˜¾ç¤ºè®¾å®šå€¼æ˜¯å¦æœ‰å˜åŒ–
            setpoint_info = ""
            if change_result['has_setpoint_change']:
                setpoint_info = f", è®¾å®šå€¼å˜åŒ–: U={setpoints[0]}A V={setpoints[1]}A W={setpoints[2]}A"
            if change_result['has_deadzone_change']:
                setpoint_info += f", æ­»åŒºå˜åŒ–: {arc_data_obj.manual_deadzone_percent}%"
            
            print(f"âœ… [DB1] å¼§æµå¼§å‹æ•°æ®å·²ç¼“å­˜: Uç›¸å¼§æµ={arc_data_obj.phase_U.current_A}A{setpoint_info}")
            
    except Exception as e:
        print(f"âŒ å¤„ç† DB1 å¼§æµå¼§å‹æ•°æ®å¤±è´¥: {e}")
        traceback.print_exc()


def process_status_data(raw_data: bytes):
    """å¤„ç† DB30 çŠ¶æ€æ•°æ® (åªç¼“å­˜ï¼Œä¸å†™å…¥æ•°æ®åº“)"""
    global _latest_status_data, _latest_status_timestamp
    
    if not _status_parser:
        return

    try:
        parsed = _status_parser.parse_all(raw_data)
        
        with _data_lock:
            _latest_status_data = parsed
            _latest_status_timestamp = datetime.now()
            
    except Exception as e:
        print(f"âŒ å¤„ç† DB30 çŠ¶æ€æ•°æ®å¤±è´¥: {e}")


def process_db41_data(raw_data: bytes):
    """å¤„ç† DB41 æ•°æ®çŠ¶æ€ (åªç¼“å­˜ï¼Œä¸å†™å…¥æ•°æ®åº“)"""
    global _latest_db41_data, _latest_db41_timestamp
    
    if not _db41_parser:
        return

    try:
        parsed = _db41_parser.parse_all(raw_data)
        
        with _data_lock:
            _latest_db41_data = parsed
            _latest_db41_timestamp = datetime.now()
            
    except Exception as e:
        print(f"âŒ å¤„ç† DB41 æ•°æ®çŠ¶æ€å¤±è´¥: {e}")


def process_weight_data(
    weight_result: Dict[str, Any],
    batch_code: str,
    is_discharging: bool = False,
    is_requesting: bool = False
):
    """å¤„ç†æ–™ä»“é‡é‡æ•°æ® (Modbus RTU + PLC æŠ•æ–™ä¿¡å·)
    
    Args:
        weight_result: read_hopper_weight() è¿”å›çš„ç»“æœ
        batch_code: å½“å‰æ‰¹æ¬¡å·
        is_discharging: %Q3.7 ç§¤æ’æ–™ä¿¡å· (True=æ­£åœ¨æŠ•æ–™)
        is_requesting: %Q4.0 ç§¤è¦æ–™ä¿¡å·
    """
    global _latest_weight_data, _latest_weight_timestamp

    try:
        # 1. æ›´æ–°å†…å­˜ç¼“å­˜
        with _data_lock:
            _latest_weight_data = weight_result
            _latest_weight_timestamp = datetime.now()
        
        # 2. å¦‚æœè¯»å–æˆåŠŸï¼Œå¤„ç†æŠ•æ–™ç´¯è®¡
        if weight_result.get('success') and weight_result.get('weight') is not None:
            weight_kg = float(weight_result['weight'])
            now = datetime.now(timezone.utc)
            
            # ========================================
            # 2.1 æŠ•æ–™ç´¯è®¡å™¨ï¼šæ·»åŠ æ•°æ®ç‚¹åˆ°é˜Ÿåˆ—
            # ========================================
            feeding_acc = get_feeding_accumulator()
            feeding_result = feeding_acc.add_measurement(
                weight_kg=weight_kg,
                is_discharging=is_discharging,
                is_requesting=is_requesting
            )
            
            # 2.2 æ£€æŸ¥æ˜¯å¦éœ€è¦è®¡ç®—æŠ•æ–™ (æ¯30ç§’)
            if feeding_result['should_calc']:
                calc_result = feeding_acc.calculate_feeding()
                print(f"ğŸ“Š æŠ•æ–™è®¡ç®—å®Œæˆ: æœ¬æ¬¡æ–°å¢ {calc_result['total_added']:.1f}kg, ç´¯è®¡ {calc_result['feeding_total']:.1f}kg")
            
            # æ›´æ–°ç¼“å­˜ä¸­çš„æŠ•æ–™æ€»é‡
            with _data_lock:
                _latest_weight_data['feeding_total'] = feeding_acc.get_feeding_total()
                _latest_weight_data['is_discharging'] = is_discharging
            
            # 2.3 è½¬æ¢ä¸º InfluxDB Point
            point_dict = {
                'measurement': 'sensor_data',
                'tags': {
                    'device_type': 'electric_furnace',
                    'module_type': 'hopper_weight',
                    'device_id': 'hopper_1',
                    'batch_code': batch_code
                },
                'fields': {
                    'net_weight': weight_kg,
                    'feeding_total': feeding_acc.get_feeding_total(),
                    'is_discharging': 1 if is_discharging else 0,
                },
                'time': now
            }
            
            _normal_buffer.append(point_dict)
            
    except Exception as e:
        print(f"âŒ å¤„ç†æ–™ä»“é‡é‡æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


# ============================================================
# æ‰¹é‡å†™å…¥ InfluxDB
# ============================================================
async def flush_arc_buffer():
    """æ‰¹é‡å†™å…¥ DB1 å¼§æµå¼§å‹ç¼“å­˜
    
    æ³¨æ„: åªæœ‰åœ¨å†¶ç‚¼è¿è¡ŒçŠ¶æ€ (is_running=True) æ—¶æ‰å†™å…¥æ•°æ®åº“
    """
    global _stats, _arc_buffer
    
    if not _arc_buffer:
        return
    
    # æ£€æŸ¥æ‰¹æ¬¡çŠ¶æ€ - åªæœ‰è¿è¡Œä¸­æ‰å†™æ•°æ®åº“
    from app.services.batch_service import get_batch_service
    batch_service = get_batch_service()
    
    if not batch_service.is_running:
        # æš‚åœæˆ–æœªå¼€å§‹å†¶ç‚¼æ—¶ï¼Œæ¸…ç©ºç¼“å­˜ä½†ä¸å†™å…¥
        skipped_count = len(_arc_buffer)
        _arc_buffer.clear()
        if skipped_count > 0:
            print(f"â¸ï¸ [DB1] è·³è¿‡å†™å…¥ {skipped_count} ä¸ªæ•°æ®ç‚¹ (çŠ¶æ€: {batch_service.state.value})")
        return
    
    dict_points_list = list(_arc_buffer)
    _arc_buffer.clear()
    
    influx_points = []
    for dp in dict_points_list:
        p = build_point(dp['measurement'], dp['tags'], dp['fields'], dp['time'])
        if p:
            influx_points.append(p)
            
    if not influx_points:
        return

    try:
        success, err = write_points_batch(influx_points)
        if success:
            _stats["successful_writes"] += len(influx_points)
            print(f"âœ… [DB1] æ‰¹é‡å†™å…¥æˆåŠŸ: {len(influx_points)} ä¸ªæ•°æ®ç‚¹")
        else:
            _stats["failed_writes"] += len(influx_points)
            print(f"âŒ [DB1] æ‰¹é‡å†™å…¥å¤±è´¥: {err}")
        
    except Exception as e:
        _stats["failed_writes"] += len(influx_points)
        print(f"âŒ [DB1] æ‰¹é‡å†™å…¥å¼‚å¸¸: {e}")


async def flush_normal_buffer():
    """æ‰¹é‡å†™å…¥ DB32/é‡é‡ç¼“å­˜
    
    æ³¨æ„: åªæœ‰åœ¨å†¶ç‚¼è¿è¡ŒçŠ¶æ€ (is_running=True) æ—¶æ‰å†™å…¥æ•°æ®åº“
    """
    global _stats, _normal_buffer
    
    if not _normal_buffer:
        return
    
    # æ£€æŸ¥æ‰¹æ¬¡çŠ¶æ€ - åªæœ‰è¿è¡Œä¸­æ‰å†™æ•°æ®åº“
    from app.services.batch_service import get_batch_service
    batch_service = get_batch_service()
    
    if not batch_service.is_running:
        # æš‚åœæˆ–æœªå¼€å§‹å†¶ç‚¼æ—¶ï¼Œæ¸…ç©ºç¼“å­˜ä½†ä¸å†™å…¥
        skipped_count = len(_normal_buffer)
        _normal_buffer.clear()
        if skipped_count > 0:
            print(f"â¸ï¸ [DB32] è·³è¿‡å†™å…¥ {skipped_count} ä¸ªæ•°æ®ç‚¹ (çŠ¶æ€: {batch_service.state.value})")
        return
    
    dict_points_list = list(_normal_buffer)
    _normal_buffer.clear()
    
    influx_points = []
    for dp in dict_points_list:
        p = build_point(dp['measurement'], dp['tags'], dp['fields'], dp['time'])
        if p:
            influx_points.append(p)
            
    if not influx_points:
        return

    try:
        success, err = write_points_batch(influx_points)
        if success:
            _stats["successful_writes"] += len(influx_points)
            print(f"âœ… [DB32] æ‰¹é‡å†™å…¥æˆåŠŸ: {len(influx_points)} ä¸ªæ•°æ®ç‚¹")
        else:
            _stats["failed_writes"] += len(influx_points)
            print(f"âŒ [DB32] æ‰¹é‡å†™å…¥å¤±è´¥: {err}")
        
    except Exception as e:
        _stats["failed_writes"] += len(influx_points)
        print(f"âŒ [DB32] æ‰¹é‡å†™å…¥å¼‚å¸¸: {e}")


# ============================================================
# ç¼“å­˜æ•°æ®è·å–å‡½æ•° (ä¾› API è°ƒç”¨)
# ============================================================
def get_latest_modbus_data() -> Dict[str, Any]:
    """è·å–æœ€æ–°çš„ DB32 ä¼ æ„Ÿå™¨æ•°æ®"""
    with _data_lock:
        return {
            'data': _latest_modbus_data.copy() if _latest_modbus_data else {},
            'timestamp': _latest_modbus_timestamp.isoformat() if _latest_modbus_timestamp else None
        }


def get_latest_arc_data() -> Dict[str, Any]:
    """è·å–æœ€æ–°çš„ DB1 å¼§æµå¼§å‹æ•°æ®"""
    with _data_lock:
        return {
            'data': _latest_arc_data.copy() if _latest_arc_data else {},
            'timestamp': _latest_arc_timestamp.isoformat() if _latest_arc_timestamp else None
        }


def get_latest_status_data() -> Dict[str, Any]:
    """è·å–æœ€æ–°çš„ DB30 é€šä¿¡çŠ¶æ€æ•°æ®"""
    with _data_lock:
        return {
            'data': _latest_status_data.copy() if _latest_status_data else {},
            'timestamp': _latest_status_timestamp.isoformat() if _latest_status_timestamp else None
        }


def get_latest_db41_data() -> Dict[str, Any]:
    """è·å–æœ€æ–°çš„ DB41 æ•°æ®çŠ¶æ€"""
    with _data_lock:
        return {
            'data': _latest_db41_data.copy() if _latest_db41_data else {},
            'timestamp': _latest_db41_timestamp.isoformat() if _latest_db41_timestamp else None
        }


def get_latest_weight_data() -> Dict[str, Any]:
    """è·å–æœ€æ–°çš„æ–™ä»“é‡é‡æ•°æ®"""
    with _data_lock:
        return {
            'data': _latest_weight_data.copy() if _latest_weight_data else {},
            'timestamp': _latest_weight_timestamp.isoformat() if _latest_weight_timestamp else None
        }


def get_latest_electricity_data() -> Dict[str, Any]:
    """è·å–æœ€æ–°çš„ç”µè¡¨æ•°æ®
    
    æ³¨æ„: å½“å‰ç‰ˆæœ¬æ— ç‹¬ç«‹ç”µè¡¨é‡‡é›†ï¼Œè¿”å›ç©ºæ•°æ®ã€‚
    ç”µåŠ›ç›¸å…³æ•°æ®è¯·ä½¿ç”¨ get_latest_arc_data() è·å–å¼§æµå¼§å‹ã€‚
    """
    return {
        'data': {
            'converted': {
                'Pt': 0.0,       # æ€»åŠŸç‡ kW (æš‚æ— )
                'Ua_0': 0.0,     # Aç›¸ç”µå‹ V (æš‚æ— )
                'I_0': 0.0,      # Aç›¸ç”µæµ A (æš‚æ— )
                'I_1': 0.0,      # Bç›¸ç”µæµ A (æš‚æ— )
                'I_2': 0.0,      # Cç›¸ç”µæµ A (æš‚æ— )
                'ImpEp': 0.0,    # ç´¯è®¡ç”µèƒ½ kWh (æš‚æ— )
            },
            'summary': {},
            'ct_ratio': 20,
        },
        'timestamp': None
    }


def get_valve_status_queues() -> Dict[int, List[Dict[str, Any]]]:
    """è·å–4ä¸ªè¶é˜€çš„çŠ¶æ€é˜Ÿåˆ—"""
    with _data_lock:
        result = {}
        for valve_id in range(1, 5):
            status_list = list(_valve_status_queues[valve_id])
            timestamp_list = list(_valve_status_timestamps[valve_id])
            
            result[valve_id] = [
                {
                    "status": status,
                    "timestamp": ts,
                    "state_name": _parse_valve_state_name(status)
                }
                for status, ts in zip(status_list, timestamp_list)
            ]
        
        return result


def _parse_valve_state_name(status: str) -> str:
    """è§£æè¶é˜€çŠ¶æ€åç§°"""
    state_map = {
        "10": "closed",
        "01": "open",
        "11": "error",
        "00": "unknown"
    }
    return state_map.get(status, "unknown")


def get_buffer_status() -> Dict[str, Any]:
    """è·å–ç¼“å­˜çŠ¶æ€"""
    return {
        'arc_buffer_size': len(_arc_buffer),
        'normal_buffer_size': len(_normal_buffer),
        'arc_batch_size': _arc_batch_size,
        'normal_batch_size': _normal_batch_size,
        'stats': _stats.copy()
    }


def update_stats(key: str, value: Any):
    """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
    global _stats
    _stats[key] = value
