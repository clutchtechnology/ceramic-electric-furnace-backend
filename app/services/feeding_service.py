# ============================================================
# æ–‡ä»¶è¯´æ˜: feeding_service.py - æŠ•æ–™è®°å½•è®¡ç®—æœåŠ¡
# ============================================================
# åŠŸèƒ½:
#   1. å®šæ—¶è®¡ç®—æŠ•æ–™è®°å½• (æ¯20åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡)
#   2. ä» InfluxDB æŸ¥è¯¢å½“å‰æ‰¹æ¬¡çš„æ–™ä»“é‡é‡å†å²æ•°æ®
#   3. åˆ†æé‡é‡å˜åŒ–ï¼Œæ£€æµ‹æŠ•æ–™äº‹ä»¶ (é‡é‡ä¸Šå‡)
#   4. ç‰¹æ®Šå¤„ç†é¦–é‡å’Œå°¾é‡
#   5. æŠ•æ–™è®°å½•å­˜å…¥ InfluxDB çš„ feeding_records measurement
#   6. æä¾›æŸ¥è¯¢å½“å‰æ‰¹æ¬¡æŠ•æ–™æ€»é‡çš„æ¥å£
# ============================================================

import asyncio
import threading
from datetime import datetime, timezone, timedelta
from typing import Optional, Dict, Any, List
from dataclasses import dataclass

from config import get_settings
from app.core.influxdb import get_influx_client, query_data

settings = get_settings()

# ============================================================
# æ•°æ®ç»“æ„å®šä¹‰
# ============================================================
@dataclass
class FeedingRecord:
    """æŠ•æ–™è®°å½•"""
    time: datetime
    added_weight: float  # æŠ•å…¥é‡é‡ (kg)
    batch_code: str      # æ‰¹æ¬¡å·
    is_first: bool = False  # æ˜¯å¦ä¸ºé¦–é‡
    is_last: bool = False   # æ˜¯å¦ä¸ºå°¾é‡ (å¯èƒ½è¢«è¦†ç›–)


# ============================================================
# æŠ•æ–™è®¡ç®—é…ç½®
# ============================================================
FEEDING_THRESHOLD_KG = 10.0     # æŠ•æ–™æ£€æµ‹é˜ˆå€¼ (kg)
AGGREGATION_INTERVAL = "5m"     # èšåˆé—´éš”
CALCULATION_INTERVAL_MINUTES = 20  # è®¡ç®—å‘¨æœŸ (åˆ†é’Ÿ)
TIME_GAP_THRESHOLD_SECONDS = 300   # æ—¶é—´æ–­æ¡£é˜ˆå€¼ (ç§’ï¼Œ5åˆ†é’Ÿ)


# ============================================================
# æ¨¡å—çº§ç¼“å­˜
# ============================================================
_feeding_lock = threading.Lock()
_last_calculation_time: Optional[datetime] = None
_current_batch_feeding_total: float = 0.0  # å½“å‰æ‰¹æ¬¡æŠ•æ–™æ€»é‡ç¼“å­˜


def calculate_feeding_records(
    batch_code: str,
    start_time: datetime,
    end_time: Optional[datetime] = None
) -> List[FeedingRecord]:
    """è®¡ç®—æŒ‡å®šæ‰¹æ¬¡çš„æŠ•æ–™è®°å½•
    
    Args:
        batch_code: æ‰¹æ¬¡å·
        start_time: æ‰¹æ¬¡å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´ (None è¡¨ç¤ºå½“å‰æ—¶é—´)
        
    Returns:
        æŠ•æ–™è®°å½•åˆ—è¡¨
    """
    if end_time is None:
        end_time = datetime.now(timezone.utc)
    
    # ç¡®ä¿æ—¶é—´å¸¦æ—¶åŒº
    if start_time.tzinfo is None:
        start_time = start_time.replace(tzinfo=timezone.utc)
    if end_time.tzinfo is None:
        end_time = end_time.replace(tzinfo=timezone.utc)
    
    print(f"ğŸ“Š å¼€å§‹è®¡ç®—æ‰¹æ¬¡ {batch_code} çš„æŠ•æ–™è®°å½•...")
    print(f"   æ—¶é—´èŒƒå›´: {start_time.isoformat()} ~ {end_time.isoformat()}")
    
    # 1. æŸ¥è¯¢æ–™ä»“é‡é‡å†å²æ•°æ® (5åˆ†é’Ÿèšåˆ)
    weight_data = _query_weight_history(start_time, end_time)
    
    if not weight_data:
        print(f"   âš ï¸ æ— é‡é‡å†å²æ•°æ®")
        return []
    
    print(f"   ğŸ“ˆ è·å–åˆ° {len(weight_data)} æ¡é‡é‡æ•°æ®ç‚¹")
    
    # 2. åˆ†ææŠ•æ–™äº‹ä»¶
    feeding_records = _analyze_feeding_events(weight_data, batch_code)
    
    print(f"   âœ… æ£€æµ‹åˆ° {len(feeding_records)} æ¬¡æŠ•æ–™äº‹ä»¶")
    
    return feeding_records


def _query_weight_history(
    start_time: datetime, 
    end_time: datetime
) -> List[Dict[str, Any]]:
    """æŸ¥è¯¢æ–™ä»“é‡é‡å†å²æ•°æ®
    
    Args:
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        
    Returns:
        é‡é‡æ•°æ®ç‚¹åˆ—è¡¨ [{"time": datetime, "value": float}, ...]
    """
    client = get_influx_client()
    query_api = client.query_api()
    bucket = settings.influx_bucket
    
    # Flux æŸ¥è¯¢ï¼šè·å– weight å­—æ®µï¼ŒæŒ‰5åˆ†é’Ÿèšåˆå–å¹³å‡å€¼
    query = f'''
    from(bucket: "{bucket}")
        |> range(start: {start_time.isoformat()}, stop: {end_time.isoformat()})
        |> filter(fn: (r) => r["_measurement"] == "sensor_data")
        |> filter(fn: (r) => r["device_id"] == "hopper_1")
        |> filter(fn: (r) => r["_field"] == "weight")
        |> aggregateWindow(every: 5m, fn: mean, createEmpty: false)
        |> sort(columns: ["_time"], desc: false)
    '''
    
    try:
        result = query_api.query(query)
        
        points = []
        for table in result:
            for record in table.records:
                points.append({
                    "time": record.get_time(),
                    "value": float(record.get_value()) if record.get_value() is not None else 0.0
                })
        
        return points
        
    except Exception as e:
        print(f"   âŒ æŸ¥è¯¢é‡é‡å†å²æ•°æ®å¤±è´¥: {e}")
        return []


def _analyze_feeding_events(
    weight_data: List[Dict[str, Any]], 
    batch_code: str
) -> List[FeedingRecord]:
    """åˆ†æé‡é‡æ•°æ®ï¼Œæ£€æµ‹æŠ•æ–™äº‹ä»¶
    
    æŠ•æ–™æ£€æµ‹é€»è¾‘:
    1. é¦–é‡: å¦‚æœæ‰¹æ¬¡å¼€å§‹æ—¶å°±æœ‰é‡é‡ > é˜ˆå€¼ï¼Œè®°å½•ä¸ºé¦–æ¬¡æŠ•æ–™
    2. ä¸­é—´: æ£€æµ‹é‡é‡ä¸Šå‡æ²¿ (å½“å‰å€¼ - ä¸Šä¸€æ¬¡å€¼ > é˜ˆå€¼)
    3. å°¾é‡: å¦‚æœæœ€åä¸€ä¸ªæ•°æ®ç‚¹ç›¸æ¯”å‰ä¸€ä¸ªæ˜¯ä¸Šå‡çš„ï¼Œä¹Ÿè®°å½• (å¯èƒ½è¢«åç»­è¦†ç›–)
    
    Args:
        weight_data: é‡é‡æ•°æ®ç‚¹åˆ—è¡¨
        batch_code: æ‰¹æ¬¡å·
        
    Returns:
        æŠ•æ–™è®°å½•åˆ—è¡¨
    """
    if len(weight_data) < 1:
        return []
    
    feeding_records: List[FeedingRecord] = []
    
    # 1. é¦–é‡å¤„ç†: å¦‚æœç¬¬ä¸€ä¸ªæ•°æ®ç‚¹é‡é‡ > é˜ˆå€¼ï¼Œè§†ä¸ºæ‰¹æ¬¡å¼€å§‹æ—¶å·²æœ‰æ–™
    first_point = weight_data[0]
    if first_point["value"] > FEEDING_THRESHOLD_KG:
        feeding_records.append(FeedingRecord(
            time=first_point["time"],
            added_weight=first_point["value"],
            batch_code=batch_code,
            is_first=True
        ))
        print(f"      é¦–é‡: {first_point['value']:.2f} kg at {first_point['time']}")
    
    # 2. ä¸­é—´æ£€æµ‹: éå†æ•°æ®ç‚¹ï¼Œæ£€æµ‹ä¸Šå‡æ²¿
    prev_point = weight_data[0]
    
    for i in range(1, len(weight_data)):
        curr_point = weight_data[i]
        curr_val = curr_point["value"]
        prev_val = prev_point["value"]
        curr_time = curr_point["time"]
        prev_time = prev_point["time"]
        
        # æ—¶é—´é—´éš”æ£€æŸ¥ (é¿å…æ–­æ¡£æ•°æ®å¹²æ‰°)
        time_diff = (curr_time - prev_time).total_seconds()
        if time_diff > TIME_GAP_THRESHOLD_SECONDS:
            # æ—¶é—´æ–­æ¡£ï¼Œé‡ç½®åŸºå‡†
            prev_point = curr_point
            continue
        
        # è®¡ç®—é‡é‡å˜åŒ–
        diff = curr_val - prev_val
        
        # æ£€æµ‹ä¸Šå‡æ²¿ (æŠ•æ–™äº‹ä»¶)
        if diff > FEEDING_THRESHOLD_KG:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªç‚¹
            is_last = (i == len(weight_data) - 1)
            
            feeding_records.append(FeedingRecord(
                time=curr_time,
                added_weight=diff,
                batch_code=batch_code,
                is_last=is_last
            ))
            
            if is_last:
                print(f"      å°¾é‡(å¾…ç¡®è®¤): +{diff:.2f} kg at {curr_time}")
            else:
                print(f"      æŠ•æ–™: +{diff:.2f} kg at {curr_time}")
        
        prev_point = curr_point
    
    return feeding_records


def save_feeding_records(records: List[FeedingRecord]) -> bool:
    """å°†æŠ•æ–™è®°å½•ä¿å­˜åˆ° InfluxDB
    
    å¯¹äº is_last=True çš„è®°å½•ï¼Œä¼šå…ˆåˆ é™¤è¯¥æ‰¹æ¬¡çš„å°¾é‡è®°å½•ï¼Œå†å†™å…¥æ–°çš„
    (å®ç°è¦†ç›–é€»è¾‘)
    
    Args:
        records: æŠ•æ–™è®°å½•åˆ—è¡¨
        
    Returns:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    if not records:
        return True
    
    from influxdb_client import Point
    from influxdb_client.client.write_api import SYNCHRONOUS
    
    client = get_influx_client()
    write_api = client.write_api(write_options=SYNCHRONOUS)
    bucket = settings.influx_bucket
    
    try:
        # åˆ†ç¦»å°¾é‡è®°å½•å’Œæ™®é€šè®°å½•
        last_records = [r for r in records if r.is_last]
        normal_records = [r for r in records if not r.is_last]
        
        # å¤„ç†å°¾é‡è®°å½• (åˆ é™¤æ—§çš„ï¼Œå†™å…¥æ–°çš„)
        for record in last_records:
            # å…ˆåˆ é™¤è¯¥æ‰¹æ¬¡çš„æ—§å°¾é‡è®°å½•
            _delete_last_feeding_record(record.batch_code)
        
        # æ„å»º Point å¯¹è±¡
        points = []
        for record in records:
            p = Point("feeding_records") \
                .tag("batch_code", record.batch_code) \
                .tag("device_id", "hopper_1") \
                .tag("is_first", str(record.is_first).lower()) \
                .tag("is_last", str(record.is_last).lower()) \
                .field("added_weight", record.added_weight) \
                .time(record.time)
            points.append(p)
        
        # æ‰¹é‡å†™å…¥
        write_api.write(bucket=bucket, record=points)
        print(f"   ğŸ’¾ å·²ä¿å­˜ {len(points)} æ¡æŠ•æ–™è®°å½•")
        return True
        
    except Exception as e:
        print(f"   âŒ ä¿å­˜æŠ•æ–™è®°å½•å¤±è´¥: {e}")
        return False


def _delete_last_feeding_record(batch_code: str) -> bool:
    """åˆ é™¤æŒ‡å®šæ‰¹æ¬¡çš„å°¾é‡è®°å½•
    
    æ³¨æ„: InfluxDB åˆ é™¤æ“ä½œéœ€è¦é€šè¿‡ delete API
    ç”±äº InfluxDB çš„ç‰¹æ€§ï¼Œè¿™é‡Œé‡‡ç”¨æ ‡è®°åˆ é™¤çš„æ–¹å¼
    
    Args:
        batch_code: æ‰¹æ¬¡å·
        
    Returns:
        æ˜¯å¦åˆ é™¤æˆåŠŸ
    """
    # å®é™…å®ç°ä¸­ï¼Œå¯ä»¥é€šè¿‡ InfluxDB Delete API åˆ é™¤
    # æˆ–è€…åœ¨æŸ¥è¯¢æ—¶è¿‡æ»¤æ‰ is_last=true çš„æ—§è®°å½•
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œæ–°è®°å½•ä¼šè‡ªåŠ¨è¦†ç›–åŒæ—¶é—´ç‚¹çš„æ—§è®°å½•
    print(f"   ğŸ—‘ï¸ æ ‡è®°åˆ é™¤æ‰¹æ¬¡ {batch_code} çš„æ—§å°¾é‡è®°å½•")
    return True


def get_batch_feeding_total(batch_code: str, start_time: datetime) -> float:
    """è·å–æŒ‡å®šæ‰¹æ¬¡çš„æŠ•æ–™æ€»é‡
    
    Args:
        batch_code: æ‰¹æ¬¡å·
        start_time: æ‰¹æ¬¡å¼€å§‹æ—¶é—´
        
    Returns:
        æŠ•æ–™æ€»é‡ (kg)
    """
    from datetime import timezone
    
    client = get_influx_client()
    query_api = client.query_api()
    bucket = settings.influx_bucket
    
    # ç¡®ä¿æ—¶é—´å¸¦æ—¶åŒº
    if start_time.tzinfo is None:
        start_time = start_time.replace(tzinfo=timezone.utc)
    
    # [FIX] æ£€æŸ¥æ‰¹æ¬¡åˆšå¼€å§‹çš„æƒ…å†µï¼Œé¿å…ç©ºèŒƒå›´æŸ¥è¯¢
    now = datetime.now(timezone.utc)
    if start_time > now:
        # æ‰¹æ¬¡å¼€å§‹æ—¶é—´åœ¨æœªæ¥ï¼ˆæ—¶é’ŸåŒæ­¥é—®é¢˜ï¼‰ï¼Œè¿”å›0
        return 0.0
    
    # [FIX] å¦‚æœæ‰¹æ¬¡åˆšå¼€å§‹ä¸åˆ°2ç§’ï¼Œç›´æ¥è¿”å›0ï¼Œé¿å…é¢‘ç¹æŸ¥è¯¢
    if (now - start_time).total_seconds() < 2:
        return 0.0
    
    # Flux æŸ¥è¯¢ï¼šè·å–è¯¥æ‰¹æ¬¡çš„æ‰€æœ‰æŠ•æ–™è®°å½•å¹¶æ±‚å’Œ
    query = f'''
    from(bucket: "{bucket}")
        |> range(start: {start_time.isoformat()})
        |> filter(fn: (r) => r["_measurement"] == "feeding_records")
        |> filter(fn: (r) => r["batch_code"] == "{batch_code}")
        |> filter(fn: (r) => r["_field"] == "added_weight")
        |> sum()
    '''
    
    try:
        result = query_api.query(query)
        
        total = 0.0
        for table in result:
            for record in table.records:
                total = float(record.get_value()) if record.get_value() else 0.0
        
        return total
        
    except Exception as e:
        print(f"   âŒ æŸ¥è¯¢æŠ•æ–™æ€»é‡å¤±è´¥: {e}")
        return 0.0


def get_batch_feeding_records(batch_code: str, start_time: datetime) -> List[Dict[str, Any]]:
    """è·å–æŒ‡å®šæ‰¹æ¬¡çš„æ‰€æœ‰æŠ•æ–™è®°å½•
    
    Args:
        batch_code: æ‰¹æ¬¡å·
        start_time: æ‰¹æ¬¡å¼€å§‹æ—¶é—´
        
    Returns:
        æŠ•æ–™è®°å½•åˆ—è¡¨
    """
    client = get_influx_client()
    query_api = client.query_api()
    bucket = settings.influx_bucket
    
    # ç¡®ä¿æ—¶é—´å¸¦æ—¶åŒº
    if start_time.tzinfo is None:
        start_time = start_time.replace(tzinfo=timezone.utc)
    
    query = f'''
    from(bucket: "{bucket}")
        |> range(start: {start_time.isoformat()})
        |> filter(fn: (r) => r["_measurement"] == "feeding_records")
        |> filter(fn: (r) => r["batch_code"] == "{batch_code}")
        |> filter(fn: (r) => r["_field"] == "added_weight")
        |> sort(columns: ["_time"], desc: false)
    '''
    
    try:
        result = query_api.query(query)
        
        records = []
        for table in result:
            for record in table.records:
                records.append({
                    "time": record.get_time().isoformat(),
                    "added_weight": float(record.get_value()) if record.get_value() else 0.0,
                    "is_first": record.values.get("is_first", "false") == "true",
                    "is_last": record.values.get("is_last", "false") == "true",
                })
        
        return records
        
    except Exception as e:
        print(f"   âŒ æŸ¥è¯¢æŠ•æ–™è®°å½•å¤±è´¥: {e}")
        return []


# ============================================================
# å®šæ—¶è®¡ç®—ä»»åŠ¡
# ============================================================
async def run_feeding_calculation_task(
    get_batch_info_func,
    interval_minutes: int = CALCULATION_INTERVAL_MINUTES
):
    """å®šæ—¶æŠ•æ–™è®¡ç®—ä»»åŠ¡
    
    æ¯ interval_minutes åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡æŠ•æ–™è®¡ç®—
    
    Args:
        get_batch_info_func: è·å–å½“å‰æ‰¹æ¬¡ä¿¡æ¯çš„å‡½æ•°
        interval_minutes: è®¡ç®—é—´éš” (åˆ†é’Ÿ)
    """
    global _last_calculation_time, _current_batch_feeding_total
    
    print(f"ğŸ”„ æŠ•æ–™è®¡ç®—å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ (é—´éš”: {interval_minutes} åˆ†é’Ÿ)")
    
    while True:
        try:
            await asyncio.sleep(interval_minutes * 60)
            
            # è·å–å½“å‰æ‰¹æ¬¡ä¿¡æ¯
            batch_info = get_batch_info_func()
            batch_code = batch_info.get('batch_code')
            start_time_str = batch_info.get('start_time')
            
            if not batch_code or not start_time_str:
                print("â¸ï¸ æ— æ´»åŠ¨æ‰¹æ¬¡ï¼Œè·³è¿‡æŠ•æ–™è®¡ç®—")
                continue
            
            # è§£æå¼€å§‹æ—¶é—´
            start_time = datetime.fromisoformat(start_time_str)
            
            with _feeding_lock:
                _last_calculation_time = datetime.now()
            
            # è®¡ç®—æŠ•æ–™è®°å½•
            records = calculate_feeding_records(batch_code, start_time)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            if records:
                save_feeding_records(records)
                
                # æ›´æ–°ç¼“å­˜çš„æŠ•æ–™æ€»é‡
                total = sum(r.added_weight for r in records)
                with _feeding_lock:
                    _current_batch_feeding_total = total
                    
                print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_code} å½“å‰æŠ•æ–™æ€»é‡: {total:.2f} kg")
            
        except asyncio.CancelledError:
            print("ğŸ›‘ æŠ•æ–™è®¡ç®—å®šæ—¶ä»»åŠ¡å·²åœæ­¢")
            break
        except Exception as e:
            print(f"âŒ æŠ•æ–™è®¡ç®—ä»»åŠ¡å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()


def get_cached_feeding_total() -> float:
    """è·å–ç¼“å­˜çš„å½“å‰æ‰¹æ¬¡æŠ•æ–™æ€»é‡
    
    Returns:
        æŠ•æ–™æ€»é‡ (kg)
    """
    with _feeding_lock:
        return _current_batch_feeding_total


def trigger_feeding_calculation(batch_code: str, start_time: datetime) -> float:
    """æ‰‹åŠ¨è§¦å‘æŠ•æ–™è®¡ç®— (ç”¨äºAPIè°ƒç”¨)
    
    Args:
        batch_code: æ‰¹æ¬¡å·
        start_time: æ‰¹æ¬¡å¼€å§‹æ—¶é—´
        
    Returns:
        æŠ•æ–™æ€»é‡ (kg)
    """
    global _current_batch_feeding_total
    
    records = calculate_feeding_records(batch_code, start_time)
    
    if records:
        save_feeding_records(records)
        total = sum(r.added_weight for r in records)
        
        with _feeding_lock:
            _current_batch_feeding_total = total
            
        return total
    
    return 0.0
